# Chat Log: Implementing Memory and Context Retrieval for Claude Code

## User Message
# Implementing Memory and Context Retrieval for Claude Code

I'm using Claude Code, and I want it to have memory and the ability to retrieve contextual data about my code: reading files, documentation, etc. One of the ways to implement it is using MCP and a vector database (am I right?). What other ways are there to implement it?

Below is a map of the main design patterns teams are using today to give **Claude Code** (or any other LLM) persistent, code-aware memory. They all start with the same two building blocks—_chunking your repo/docs_ and _storing metadata about each chunk_—but differ in how they **retrieve** and **evolve** that memory over time.

---

## 1. Plain RAG with a Vector DB (the baseline you know)

- **Pipeline**: split files ➜ embed ➜ store in Pinecone/Weaviate/Chroma, etc. ➜ cosine‑similarity retrieval ➜ stuff top‑k into the prompt.
- **Strengths**: simple, stateless, easy to scale horizontally.
- **Limits**: no notion of "importance", "recency" or code structure; large snippets can blow the context budget; dense retrieval sometimes misses exact-match identifiers.

---

## 2. Hybrid or "Sparse + Dense" Retrieval

- Combine BM25/keyword or TF‑IDF scores with dense‑vector similarity to find both _exact_ symbol names and _semantic_ matches.
- Many LangChain retriever classes already expose `hybrid_search()` or self‑hosted BM25.

---

## 3. Code-Aware Embeddings & AST Indexing

- Models trained on code (e.g. OpenAI's code‑search‑ada) embed function names, docstrings and call graphs more faithfully than general‑text models.
- Layer on an AST or ctags index so you can answer structural queries ("who calls `init_db`?") with exact line numbers before falling back to vectors.

---

## 4. Contextual or "Chunk + Summary" Embedding

- Before embedding, generate a mini‑summary _for each chunk_, then embed **\[summary || chunk text\]**.
- This LlamaIndex "DocumentContextExtractor" trick significantly boosts recall for long files or READMEs.

---

## 5. Tiered‑Memory Agents (MemGPT‑style)

- **MemGPT** treats the LLM context like RAM and pages older messages/chunks to external storage, automatically fetching them back on demand.
- You can slot your repo embeddings in as the "external context" layer and let the agent decide _when_ to pull them back in.
- Ideal for interactive coding sessions where Claude must remember decisions made hours or days ago.

---

## 6. Knowledge‑Graph Memory (Zep / Graphiti)

- Instead of flat vectors, **Zep** builds a temporal knowledge graph: nodes for code entities, docs and conversation episodes; edges carry timestamps and semantic relations.
- Retrieval happens via graph queries + optional reranking, giving time‑aware answers ("use the config _after_ the April refactor").
- Works best when you want to reason about evolving state (feature flags, version bumps, deprecations).

---

## 7. Rolling, Recursive Summarisation

- Keep only the last *N* messages plus a running summary of everything older, à la many chat UIs.
- Recent research on **Context‑Aware Hierarchical Merging** shows how to do multi‑stage summaries while minimizing hallucinations.
- Pair this with a small SQLite store to persist summaries between sessions.

---

## 8. Retrieval‑Optimised Code Search Platforms

| Tool | Memory mechanism | Notes |
|------|------------------|-------|
| **Sourcegraph Cody** | Repo‑wide embeddings stored in S3/GCS; fast semantic search | Drop‑in if you already use Sourcegraph. |
| **Claude Memory (MCP)** | Local JSON/SQLite store exposed via Anthropic Model Context Protocol | Lightweight, self‑hosted. |

These can be used _as‑is_ or cannibalised for their indexing code.

---

## 9. Fine‑Tuning / Adapters (When RAG Isn't Enough)

- If your codebase is relatively stable and private, LoRA‑style fine‑tuning on curated Q‑A pairs can bake _core APIs and team conventions_ into the model weights.
- Retrieval can then focus on the truly dynamic parts (e.g., new feature branches).

---

## 10. Practical Glue Code

- **Watchers**: trigger re‑chunk/embedding when files change (git hook or fsnotify).
- **Metadata**: store path, language, last commit, owner → allows scoped queries ("only show Java changes touched since last release").
- **Prompt budget**: rank retrieved chunks by _token‑cost × relevance_; drop or summarise low‑value hits before sending to Claude.

---

# Choosing a Stack

| Need | Lean Choice |
|------|-------------|
| Quick PoC | Vector DB + RAG |
| Large monorepo with latency needs | Hybrid retrieval with code‑specific embeddings |
| Multi‑session chat agent | MemGPT or MCP |
| Frequent schema evolution / state reasoning | Zep knowledge‑graph |
| Compliance with existing Sourcegraph infra | Cody embeddings |

---

# Take‑Away

Vector databases are just one (important) piece. You can mix‑and‑match **retrievers, summarizers, tiered memory managers, and knowledge‑graphs** to balance recall, latency and long‑term reasoning. Start simple (dense + sparse search), add code‑aware embeddings, and graduate to MemGPT‑ or Zep‑style architectures as your assistant's responsibilities—and memory footprint—grow.

Read and sumary §s

---

## Assistant Response

## Summary of Memory Implementation Approaches

Yes, MCP with a vector database is one approach, but there are many alternatives:

### **Core Approaches:**

1. **Vector DB RAG** - Simple embedding + retrieval (Pinecone, Chroma)
2. **Hybrid Search** - Combines keyword (BM25) + semantic search for better code matching
3. **Code-Aware Systems** - AST indexing + code-specific embeddings (OpenAI code-search-ada)
4. **Tiered Memory** - MemGPT-style with context as RAM, external storage as disk
5. **Knowledge Graphs** - Zep/Graphiti temporal graphs for evolving codebases
6. **Rolling Summaries** - Recursive summarization for long conversations
7. **MCP** - Anthropic's lightweight local storage protocol

### **Key Differentiators:**

- **Retrieval accuracy**: Hybrid > Pure vector DB for exact symbol names
- **Context efficiency**: Tiered memory manages token budgets better
- **Temporal reasoning**: Knowledge graphs track changes over time
- **Setup complexity**: MCP/RAG simpler than MemGPT/Zep

### **Recommendations:**

- **Quick start**: Vector DB + hybrid retrieval
- **Large codebases**: Code-aware embeddings + AST indexing
- **Long sessions**: MemGPT or MCP for conversation persistence
- **Evolving projects**: Zep for tracking state changes

The choice depends on your specific needs: codebase size, session length, and whether you need temporal reasoning.
